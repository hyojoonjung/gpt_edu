{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar -zxvf /content/drive/MyDrive/korean-english-park.dev.tar.gz\n",
    "! tar -zxvf /content/drive/MyDrive/korean-english-park.test.tar.gz\n",
    "! tar -zxvf /content/drive/MyDrive/korean-english-park.train.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_file_path = '/content/korean-english-park.train.ko'\n",
    "e_file_path = '/content/korean-english-park.train.en'\n",
    "\n",
    "with open(k_file_path, 'r' ) as f:\n",
    "    ko_raw = f.read().splitlines()\n",
    "\n",
    "with open(e_file_path, 'r' ) as f:\n",
    "    en_raw = f.read().splitlines()\n",
    "\n",
    "print(ko_raw[:3])\n",
    "print(en_raw[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'([?!,.\"])', r' \\1 ',sentence) # 특수문자 인정\n",
    "    sentence = re.sub(r'[^A-zㄱ-ㅎㅏ-ㅣ가-힣0-9?!,.\"]', ' ', sentence) # 영어, 한국어, 숫자 표현만 인정\n",
    "    sentence = re.sub(r'[\" \"]+', ' ',sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(kor,eng):\n",
    "    assert len(kor) == len(eng)\n",
    "    print(' 데이터 수  :', len(kor))\n",
    "\n",
    "    dataset = set()\n",
    "    for i , j in tqdm(list(zip(kor, eng))):\n",
    "        i = preprocess_sentence(i)\n",
    "        j = preprocess_sentence(j)\n",
    "        dataset.add((i,j))\n",
    "    print(len(dataset))\n",
    "    cleaned_corpus = list(dataset)\n",
    "    return cleaned_corpus\n",
    "# 데이터불러 오고 ->정규표현식 -> 중복 데이터\n",
    "# 좋은데이터  = 1 . 많고, 2 . 카테고리 혼동 X ,3.다양하게\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = clean_corpus(ko_raw, en_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokenizer(corpus, vocab_size, lang='ko',\n",
    "                       pad_id =0,\n",
    "                       bos_id  = 1, # 문장 시작\n",
    "                       eos_id = 2, # 문장 끝\n",
    "                       unk_id = 3, # unkown token\n",
    "                       model_type='bpe'):\n",
    "    file = './%s_corpus.txt' %  lang\n",
    "    model = \"./%s_spm\" % lang\n",
    "\n",
    "    with open(file , 'w') as f:\n",
    "        for row in corpus: f.write(str(row) + '\\n')\n",
    "\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=file, model_prefix = model, vocab_size=vocab_size,\n",
    "        pad_id=pad_id, bos_id=bos_id,eos_id=eos_id,unk_id=unk_id,\n",
    "        model_type=model_type\n",
    "    )\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load('%s.model'%model)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor, eng = zip(*cleaned_corpus)\n",
    "print(kor[0])\n",
    "vocab_size = 10000\n",
    "ko_tokenizer = generate_tokenizer(kor, vocab_size)\n",
    "en_tokenzier = generate_tokenizer(eng, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역전 문장\n",
    "# 번역후 문장\n",
    "# <Start>번역전문장<end><start>번역후문장<end>\n",
    "# 오늘 점심 뭐 먹을까?\n",
    "#<start>오늘 점심 뭐 먹을까?<end><start>오늘 점심은 식당에서 알아서 드세요 <end> dslkdfdfsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어를 맞춰보고 싶으므로 한국어 토크나이저에 bos토큰, eos 토큰 추가 옵션\n",
    "\n",
    "ko_tokenizer.SetEncodeExtraOptions(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_corpus(sentences, tokenizer):\n",
    "    corpus = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        tokens = tokenizer.encode_as_ids(sentence)\n",
    "        corpus.append(tokens)\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_corpus = make_corpus(kor, ko_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_corpus = make_corpus(eng, en_tokenzier)\n",
    "# 전처리 데이터불러 오고 ->정규표현식 -> 중복 데이터 -> 토큰화 -> 길이\n",
    "#14:02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kor[1])\n",
    "print(ko_corpus[1])\n",
    "print(eng[1])\n",
    "print(en_corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_word(corpus):\n",
    "    length_sen = [0] * len(corpus)\n",
    "    for i, j in enumerate(corpus):\n",
    "        length_sen[i] = len(j)\n",
    "    return length_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_word(ko_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def make_graph(length_sen ,title=None):\n",
    "    num_num = Counter(length_sen)\n",
    "    plt.figure(figsize=(16,10))\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.bar(range(len(num_num)), [num_num[i] for i in range(len(num_num))],)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_graph(num_of_word(ko_corpus) , 'korean')\n",
    "make_graph(num_of_word(en_corpus) , 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 80\n",
    "en_ndarray = tf.keras.preprocessing.sequence.pad_sequences(en_corpus, maxlen=MAX_LENGTH,\n",
    "                                                  truncating='post',\n",
    "                                                  padding='post')\n",
    "ko_ndarray = tf.keras.preprocessing.sequence.pad_sequences(ko_corpus, maxlen=MAX_LENGTH,\n",
    "                                                  truncating='post',\n",
    "                                                  padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ndarray[:5]\n",
    "ko_ndarray[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_k_file_path= '/content/korean-english-park.dev.ko'\n",
    "val_e_file_path = '/content/korean-english-park.dev.en'\n",
    "\n",
    "with open(val_k_file_path, 'r') as f:\n",
    "    val_ko_raw = f.read().splitlines()\n",
    "with open(val_e_file_path, 'r') as f:\n",
    "    val_en_raw = f.read().splitlines()\n",
    "\n",
    "val_cleaned_corpus  = clean_corpus(val_ko_raw, val_en_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_kor, val_eng = zip(*val_cleaned_corpus)\n",
    "val_ko_corpus = make_corpus(val_kor, ko_tokenizer)\n",
    "val_en_corpus = make_corpus(val_eng, en_tokenzier)\n",
    "\n",
    "val_en_ndarray = tf.keras.preprocessing.sequence.pad_sequences(val_en_corpus, maxlen=MAX_LENGTH,\n",
    "                                                  truncating='post',\n",
    "                                                  padding='post')\n",
    "val_ko_ndarray = tf.keras.preprocessing.sequence.pad_sequences(val_ko_corpus, maxlen=MAX_LENGTH,\n",
    "                                                  truncating='post',\n",
    "                                                  padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ko_ndarray.shape , val_en_ndarray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'enc_in' : en_ndarray ,\n",
    "                                                     'dec_in' :ko_ndarray},\n",
    "                                                    ko_ndarray)).batch(batch_size = BATCH_SIZE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({'enc_in' : val_en_ndarray , 'dec_in' :val_ko_ndarray}, val_ko_ndarray)).batch(batch_size = BATCH_SIZE)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(500).reshape(1,-1)\n",
    "np.zeros((100,10))\n",
    "for i in range(0, 100, 2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def get_angles(pos, i, d_model):\n",
    "        return   pos / 10000**(2*(i//2)/d_model)\n",
    "    pos_line = np.arange(pos).reshape(-1,1)\n",
    "    d_model_line = np.arange(d_model).reshape(1,-1)\n",
    "\n",
    "    temp_table = get_angles(pos_line, d_model_line, d_model)\n",
    "\n",
    "    sinusoid_table = np.zeros(temp_table.shape)\n",
    "\n",
    "    sinusoid_table[:,0::2] = tf.math.cos(temp_table[:, 0::2])\n",
    "    sinusoid_table[:,1::2] = tf.math.sin(temp_table[:, 1::2])\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(positional_encoding(4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, ko_tokenizer.pad_id()), 1.0, 0)\n",
    "    return seq[: , tf.newaxis, tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.num_heads = tf.cast(num_heads, tf.float32)\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.depth  = tf.cast(d_model // self.num_heads, tf.float32)\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(q,k,v,mask):\n",
    "        matmul_qk = tf.matmul(q,k,transpose_b=True)\n",
    "        matmul_qk = matmul_qk / tf.math.sqrt(self.depth)\n",
    "        activation_score = tf.keras.activations.softmax(matmul_qk,axis=-1)\n",
    "        out = tf.matmul(activation_score ,v)\n",
    "        return out , activation_score\n",
    "\n",
    "    def split_heads(self,x):\n",
    "        split_x = tf.reshape(x, (x.shape[1],self.num_heads,self.depth))\n",
    "        return tf.transpose(split_x,[0,2,1,3])\n",
    "\n",
    "    def combine_head(self, x):\n",
    "        combined_x = tf.transpose(x,[0,2,1,3])\n",
    "        return tf.reshape(combined_x, [-1, x.shape[2],self.d_model])\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        split_q = self.split_heads(Q)\n",
    "        split_k = self.split_heads(K)\n",
    "        split_v = self.split_heads(V)\n",
    "        out, attention_weight = self.scaled_dot_product_attention(split_q, split_k, split_v)\n",
    "\n",
    "        out = self.combine_head(out)\n",
    "        return out, attention_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position-wise Feed Forward Network 구현\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        #  구현\n",
    "        out = self.fc1(x)  # -> 2048\n",
    "        out = self.fc2(out) # -> 512\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.drop = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        out, enc_attn = self.enc_self_attn(x,mask)\n",
    "        out = self.drop(out)\n",
    "        x = self.norm_1(out+x) # residual + layerNormalization\n",
    "\n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        out = self.ffn(x)\n",
    "        out = self.drop(out) # dropout은 학습되는 층이 아니기 때문에 공유해서 사용합니다.\n",
    "        out = self.norm_2(out+x)\n",
    "\n",
    "        return out, enc_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderlayer = EncoderLayer(512,8,2048,0.4)\n",
    "\n",
    "input_tensor_test = np.ones((16,50,512),dtype=np.float32)\n",
    "\n",
    "mask = generate_padding_mask(np.ones((16,50),dtype=np.float32))\n",
    "\n",
    "enc_out, attn = encoderlayer(input_tensor_test, mask)\n",
    "enc_out[0], attn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 레이어 구현\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, dec_enc_mask, dec_mask):\n",
    "\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        #  구현\n",
    "        out, dec_attn = self.dec_self_attn(x,dec_mask)\n",
    "        out = self.do(out)\n",
    "        x = self.norm_1(out+x)\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        #  구현\n",
    "        out, dec_enc_attn = self.enc_dec_attn([x,enc_out,enc_out],dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        x = self.norm_2(out + x)\n",
    "\n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        #  구현\n",
    "        out = self.ffn(x)\n",
    "        out = self.do(out)\n",
    "        out = self.norm_3(out + x)\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoderlayer = DecoderLayer(512,8,2048,0.4)\n",
    "\n",
    "tgt_tensor_test = np.ones((16,50,512),dtype=np.float32)\n",
    "\n",
    "enc_mask, dec_enc_mask, dec_mask = generate_masks(np.ones((16,50),dtype=np.float32),np.ones((16,50),dtype=np.float32))\n",
    "\n",
    "dec_out, dec_attn, dec_enc_attn = decoderlayer(tgt_tensor_test, enc_out, dec_enc_mask, dec_mask)\n",
    "dec_out[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 구현\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "                        for _ in range(n_layers)]\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        #  구현\n",
    "        out = x\n",
    "        for enc_layer in self.enc_layers:\n",
    "            out, enc_attns = enc_layer(out,mask)\n",
    "\n",
    "\n",
    "        return out, enc_attns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 구현\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "                            for _ in range(n_layers)]\n",
    "\n",
    "\n",
    "    def call(self, x, enc_out, dec_enc_mask, dec_mask):\n",
    "        #  구현\n",
    "        out = x\n",
    "        for dec_layer in self.dec_layers:\n",
    "            out, dec_attns, dec_enc_attns = dec_layer(out, enc_out, dec_enc_mask, dec_mask)\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 6\n",
    "n_heads = 8\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "dropout= 0.4\n",
    "\n",
    "encoder = Encoder(n_layers=n_layers,\n",
    "                  n_heads = n_heads,\n",
    "                  d_model=d_model,\n",
    "                  d_ff=d_ff,\n",
    "                  dropout=dropout)\n",
    "decoder = Decoder(n_layers=n_layers,\n",
    "                  n_heads = n_heads,\n",
    "                  d_model=d_model,\n",
    "                  d_ff=d_ff,\n",
    "                  dropout=dropout)\n",
    "\n",
    "input_test = np.ones((16,50,512),dtype=np.float32)\n",
    "\n",
    "tgt_test = np.ones((16,50,512),dtype=np.float32)\n",
    "\n",
    "enc_mask, dec_enc_mask, dec_mask = generate_masks(np.ones((16,50),dtype=np.float32),np.ones((16,50),dtype=np.float32))\n",
    "\n",
    "enc_out, enc_attns = encoder(input_test,enc_mask)\n",
    "\n",
    "dec_out, dec_attns, dec_enc_attns = decoder(tgt_test,enc_out,dec_enc_mask, dec_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 pos_len,\n",
    "                 dropout=0.2,\n",
    "                 shared_fc=True,\n",
    "                 shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        self.train = True\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    # embedding, call 구현하기\n",
    "    def embedding(self, emb, x):\n",
    "        #  구현\n",
    "        out = emb(x) + self.pos_encoding[:x.shape[1]]\n",
    "        return out\n",
    "\n",
    "    # 모델 학습 속도, 메모리를 최적화 시키기 위해 tf.tunction()사용\n",
    "    @tf.function()\n",
    "    def call(self, x):\n",
    "        #  구현\n",
    "        enc_in, dec_in = x['enc_in'],x['dec_in']\n",
    "        enc_mask, dec_enc_mask, dec_mask = generate_masks(enc_in, dec_in)\n",
    "        inp_tensor = self.do(self.embedding(self.enc_emb, enc_in))\n",
    "        tgt_tensor = self.do(self.embedding(self.dec_emb, dec_in))\n",
    "        enc_out, enc_attns = self.encoder(inp_tensor,enc_mask)\n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(tgt_tensor,enc_out,dec_enc_mask,dec_mask)\n",
    "        logits = self.fc(dec_out)\n",
    "        if self.train:\n",
    "            return logits\n",
    "        else: return logits, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "    def train(self):\n",
    "        self.train = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 하이퍼파라미터로 Transformer 인스턴스 생성\n",
    "n_layers = 2\n",
    "n_heads = 8\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "dropout= 0.4\n",
    "src_vocab_size = 10000\n",
    "tgt_vocab_size = 10000\n",
    "pos_len = 100\n",
    "shared_fc = True\n",
    "shared_emb = False\n",
    "\n",
    "model = Transformer(n_layers=n_layers,\n",
    "                    n_heads=n_heads,\n",
    "                    d_model=d_model,\n",
    "                    d_ff=d_ff,\n",
    "                    dropout=dropout,\n",
    "                    src_vocab_size=src_vocab_size,\n",
    "                    tgt_vocab_size=tgt_vocab_size,\n",
    "                    pos_len=pos_len,\n",
    "                    shared_fc=shared_fc,\n",
    "                    shared_emb=shared_emb)\n",
    "test_data = {'enc_in':np.zeros((16,40),dtype=np.float32), 'dec_in':np.zeros((16,40),dtype=np.float32)}\n",
    "model(test_data)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler 구현\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        #  구현\n",
    "        step = tf.cast(step,tf.float32)\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_learning_rate = LearningRateScheduler(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate 인스턴스 선언 & Optimizer 구현\n",
    "\n",
    "learning_rate_scheduler = LearningRateScheduler(d_model=d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    #  구현 # real: (16,50), pred:(16,50,20000)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')(real, pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(real, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,loss=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "model.fit(train_dataset,validation_data=val_dataset,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
