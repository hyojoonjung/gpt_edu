{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'law.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, mode='r') as file:\n",
    "    #print(file.readlines())\n",
    "    for i in range(20):\n",
    "        print(file.readline() , end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = '안녕하세요 정효준  입니다 딥러닝은 어렵지 않습니다.'\n",
    "k = okt.pos(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenized = []\n",
    "with open(file_path , mode='r') as file:\n",
    "    while True:\n",
    "        line  = file.readline()\n",
    "        if not line : break\n",
    "        words = okt.pos(line, stem=True)\n",
    "        tokenized.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenized[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emb_model = Word2Vec(tokenized, vector_size=200, window = 5, min_count=2,sg=0)\n",
    "# sg = skip gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list= ['안전','사고','확인','유지','비용','국토교통부']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors =[emb_model.wv[word] for word in word_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(word_vectors)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cax = ax.matshow(similarity_matrix, cmap='coolwarm')\n",
    "import matplotlib.font_manager as fm\n",
    "manager = fm.FontManager()\n",
    "fm.fontManager.ttflist = manager.ttflist\n",
    "plt.rcParams['font.family'] = 'NanumGothic'  # 해당 글꼴이 없을 경우, 내장된 한글 글꼴로 설정하거나 나눔고딕을 설치하세요.\n",
    "plt.xticks(range(len(word_list)), word_list, rotation=45)\n",
    "plt.yticks(range(len(word_list)), word_list)\n",
    "plt.colorbar(cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_token(file_path):\n",
    "    okt = Okt()\n",
    "    with open(file_path, 'r') as fread:\n",
    "        print(file_path , '파일 을 읽고 있습니다.')\n",
    "        result = []\n",
    "        while True:\n",
    "            line = fread.readline()\n",
    "            if not line: break\n",
    "            tokenlist = okt.pos(line, stem=True, norm = True)\n",
    "            # norm True 는 normalization , 그래욬ㅋㅋ ->그래요 , ㅋㅋㅋㅋㅋ -> ㅋㅋ\n",
    "            # 잘먹고 와 ㅋ\n",
    "            for word in tokenlist:\n",
    "                if word[1] in ['Noun']:\n",
    "                    result.append((word[0]))\n",
    "        return ' '.join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf 빈도수 기반, 중요 단어 추출\n",
    "ex1 = 'this is about messi messi messi' # this 1 / 6 * log(2/2) , messi 1/2\n",
    "ex2  ='this is about tf-idf' # this 1/4 * log(2/2)\n",
    "ex3  = '빨간 원피스'\n",
    "ex4 = '빨간 티셔츠'\n",
    "ex5 = '파란 티셔츠'\n",
    "ex6 =' 파란 원피스'\n",
    "# TF : 특정 문서에서 단어가 나타난 수 / 특정 문서에 있는 전체 단어의 수\n",
    "# IDF : log(말뭉치에서의 전체 문서의 수 / 말뭉치에서 해당 단어가 나타난 문서의 수)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = read_token('/content/drive/MyDrive/text_m.txt')\n",
    "m[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vectorizer.fit_transform([m,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = x[0].tocoo()\n",
    "c = x[1].tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = []\n",
    "w2 =[]\n",
    "for i , j in zip(m.col , m.data):\n",
    "    w1.append([i,j])\n",
    "# w1 = [[i,j] for i , j in zip(m.col, m.data)]\n",
    "for i , j in zip(c.col , c.data):\n",
    "    w2.append([i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.sort(key = lambda x : x[1] , reverse=True)\n",
    "w2.sort(key = lambda x : x[1] , reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    print(vectorizer.get_feature_names_out()[w1[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    print(vectorizer.get_feature_names_out()[w2[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path  ='/content/drive/MyDrive/ko.bin'\n",
    "word2vec = gensim.models.FastText.load(word2vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model tensorflow, pytorch - transformers\n",
    "# tensorflow2 keras - google, pytorch- facebook\n",
    "x = [1,2,3,4,5]\n",
    "y = [2,4,6,8,10]\n",
    "ex_x =['안녕,하세 ,요 ']\n",
    "model = tensorflow.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tensorflow.keras.layers.Dense(3,input_shape=(1,)))\n",
    "model.add(tensorflow.keras.layers.Dense(2))\n",
    "model.add(tensorflow.keras.layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, optimizer\n",
    "# mse :\n",
    "model.compile('sgd','mse')\n",
    "model.fit(x,y , epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([0, 100, 73])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN LSTM\n",
    "!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = 'NVDA'\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfe = yf.Ticker(symbol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=pfe.history(period='2y')\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist['High']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_prices = (hist['High'].values + hist['Low'].values)/2\n",
    "mid_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_prices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 50\n",
    "mid_prices = mid_prices.reshape(-1, 1)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler((0,1))\n",
    "sc_mid_prices = sc.fit_transform(mid_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sc_mid_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "label_data = []\n",
    "for index in range(len(sc_mid_prices) - train_len):\n",
    "    train_data.append(sc_mid_prices[index:train_len + index])\n",
    "    label_data.append(sc_mid_prices[index+train_len])\n",
    "\n",
    "print(train_data)\n",
    "print(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(train_data))\n",
    "print(len(label_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_data =np.array(train_data)\n",
    "label_data = np.array(label_data)\n",
    "train_data.shape , label_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = train_data[-50:]\n",
    "test_label_data =label_data[-50:]\n",
    "train_data = train_data[:-50]\n",
    "label_data = label_data[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape, train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=train_data.shape[1:],\n",
    "          return_sequences=True , activation='relu'))\n",
    "model.add(LSTM(50,return_sequences=False , activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=15 , monitor = 'val_loss')\n",
    "mc = ModelCheckpoint('/content/Untitled Folder', save_best_only=True,\n",
    "                     monitor = 'val_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, label_data, epochs=500,\n",
    "          validation_data = (test_data,test_label_data),\n",
    "          callbacks=[es,mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "predict = model.predict(test_data)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(test_label_data, label='True')\n",
    "ax.plot(predict , label = 'predict')\n",
    "ax.legend()\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3538e3d56709e678fe958d8106e85ad5fb55bdbefb75f2902995f21e682655b"
  },
  "kernelspec": {
   "display_name": "Python 3.11.4 ('tweet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
