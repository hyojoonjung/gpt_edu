{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'law.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, mode='r') as file:\n",
    "    #print(file.readlines())\n",
    "    for i in range(20):\n",
    "        print(file.readline() , end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = '안녕하세요 정효준  입니다 딥러닝은 어렵지 않습니다.'\n",
    "k = okt.pos(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenized = []\n",
    "with open(file_path , mode='r') as file:\n",
    "    while True:\n",
    "        line  = file.readline()\n",
    "        if not line : break\n",
    "        words = okt.pos(line, stem=True)\n",
    "        tokenized.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenized[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emb_model = Word2Vec(tokenized, vector_size=200, window = 5, min_count=2,sg=0)\n",
    "# sg = skip gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list= ['안전','사고','확인','유지','비용','국토교통부']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors =[emb_model.wv[word] for word in word_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(word_vectors)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cax = ax.matshow(similarity_matrix, cmap='coolwarm')\n",
    "import matplotlib.font_manager as fm\n",
    "manager = fm.FontManager()\n",
    "fm.fontManager.ttflist = manager.ttflist\n",
    "plt.rcParams['font.family'] = 'NanumGothic'  # 해당 글꼴이 없을 경우, 내장된 한글 글꼴로 설정하거나 나눔고딕을 설치하세요.\n",
    "plt.xticks(range(len(word_list)), word_list, rotation=45)\n",
    "plt.yticks(range(len(word_list)), word_list)\n",
    "plt.colorbar(cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_token(file_path):\n",
    "    okt = Okt()\n",
    "    with open(file_path, 'r') as fread:\n",
    "        print(file_path , '파일 을 읽고 있습니다.')\n",
    "        result = []\n",
    "        while True:\n",
    "            line = fread.readline()\n",
    "            if not line: break\n",
    "            tokenlist = okt.pos(line, stem=True, norm = True)\n",
    "            # norm True 는 normalization , 그래욬ㅋㅋ ->그래요 , ㅋㅋㅋㅋㅋ -> ㅋㅋ\n",
    "            # 잘먹고 와 ㅋ\n",
    "            for word in tokenlist:\n",
    "                if word[1] in ['Noun']:\n",
    "                    result.append((word[0]))\n",
    "        return ' '.join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf 빈도수 기반, 중요 단어 추출\n",
    "ex1 = 'this is about messi messi messi' 1/ 6\n",
    "ex2  ='this is about tf-idf'\n",
    "ex3  = '빨간 원피스'\n",
    "ex4 = '빨간 티셔츠'\n",
    "# TF : 특정 문서에서 단어가 나타난 수 / 특정 문서에 있는 전체 단어의 수\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3538e3d56709e678fe958d8106e85ad5fb55bdbefb75f2902995f21e682655b"
  },
  "kernelspec": {
   "display_name": "Python 3.11.4 ('tweet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
