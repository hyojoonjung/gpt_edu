{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar -zxvf /content/drive/MyDrive/korean-english-park.dev.tar.gz\n",
    "! tar -zxvf /content/drive/MyDrive/korean-english-park.test.tar.gz\n",
    "! tar -zxvf /content/drive/MyDrive/korean-english-park.train.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_file_path = '/content/korean-english-park.train.ko'\n",
    "e_file_path = '/content/korean-english-park.train.en'\n",
    "\n",
    "with open(k_file_path, 'r' ) as f:\n",
    "    ko_raw = f.read().splitlines()\n",
    "\n",
    "with open(e_file_path, 'r' ) as f:\n",
    "    en_raw = f.read().splitlines()\n",
    "\n",
    "print(ko_raw[:3])\n",
    "print(en_raw[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'([?!,.\"])', r' \\1 ',sentence) # 특수문자 인정\n",
    "    sentence = re.sub(r'[^A-zㄱ-ㅎㅏ-ㅣ가-힣0-9?!,.\"]', ' ', sentence) # 영어, 한국어, 숫자 표현만 인정\n",
    "    sentence = re.sub(r'[\" \"]+', ' ',sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(kor,eng):\n",
    "    assert len(kor) == len(eng)\n",
    "    print(' 데이터 수  :', len(kor))\n",
    "\n",
    "    dataset = set()\n",
    "    for i , j in tqdm(list(zip(kor, eng))):\n",
    "        i = preprocess_sentence(i)\n",
    "        j = preprocess_sentence(j)\n",
    "        dataset.add((i,j))\n",
    "    print(len(dataset))\n",
    "    cleaned_corpus = list(dataset)\n",
    "    return cleaned_corpus\n",
    "# 데이터불러 오고 ->정규표현식 -> 중복 데이터\n",
    "# 좋은데이터  = 1 . 많고, 2 . 카테고리 혼동 X ,3.다양하게\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = clean_corpus(ko_raw, en_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokenizer(corpus, vocab_size, lang='ko',\n",
    "                       pad_id =0,\n",
    "                       bos_id  = 1, # 문장 시작\n",
    "                       eos_id = 2, # 문장 끝\n",
    "                       unk_id = 3, # unkown token\n",
    "                       model_type='bpe'):\n",
    "    file = './%s_corpus.txt' %  lang\n",
    "    model = \"./%s_spm\" % lang\n",
    "\n",
    "    with open(file , 'w') as f:\n",
    "        for row in corpus: f.write(str(row) + '\\n')\n",
    "\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=file, model_prefix = model, vocab_size=vocab_size,\n",
    "        pad_id=pad_id, bos_id=bos_id,eos_id=eos_id,unk_id=unk_id,\n",
    "        model_type=model_type\n",
    "    )\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load('%s.model'%model)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor, eng = zip(*cleaned_corpus)\n",
    "print(kor[0])\n",
    "vocab_size = 10000\n",
    "ko_tokenizer = generate_tokenizer(kor, vocab_size)\n",
    "en_tokenzier = generate_tokenizer(eng, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역전 문장\n",
    "# 번역후 문장\n",
    "# <Start>번역전문장<end><start>번역후문장<end>\n",
    "# 오늘 점심 뭐 먹을까?\n",
    "#<start>오늘 점심 뭐 먹을까?<end><start>오늘 점심은 식당에서 알아서 드세요 <end> dslkdfdfsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어를 맞춰보고 싶으므로 한국어 토크나이저에 bos토큰, eos 토큰 추가 옵션\n",
    "\n",
    "ko_tokenizer.SetEncodeExtraOptions(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_corpus(sentences, tokenizer):\n",
    "    corpus = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        tokens = tokenizer.encode_as_ids(sentence)\n",
    "        corpus.append(tokens)\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_corpus = make_corpus(kor, ko_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_corpus = make_corpus(eng, en_tokenzier)\n",
    "# 전처리 데이터불러 오고 ->정규표현식 -> 중복 데이터 -> 토큰화 -> 길이\n",
    "#14:02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kor[1])\n",
    "print(ko_corpus[1])\n",
    "print(eng[1])\n",
    "print(en_corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_word(corpus):\n",
    "    length_sen = [0] * len(corpus)\n",
    "    for i, j in enumerate(corpus):\n",
    "        length_sen[i] = len(j)\n",
    "    return length_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_word(ko_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def make_graph(length_sen ,title=None):\n",
    "    num_num = Counter(length_sen)\n",
    "    plt.figure(figsize=(16,10))\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.bar(range(len(num_num)), [num_num[i] for i in range(len(num_num))],)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_graph(num_of_word(ko_corpus) , 'korean')\n",
    "make_graph(num_of_word(en_corpus) , 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 80\n",
    "en_ndarray = tf.keras.preprocessing.sequence.pad_sequences(en_corpus, maxlen=MAX_LENGTH,\n",
    "                                                  truncating='post',\n",
    "                                                  padding='post')\n",
    "ko_ndarray = tf.keras.preprocessing.sequence.pad_sequences(ko_corpus, maxlen=MAX_LENGTH,\n",
    "                                                  truncating='post',\n",
    "                                                  padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ndarray[:5]\n",
    "ko_ndarray[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_k_file_path= '/content/korean-english-park.dev.ko'\n",
    "val_e_file_path = '/content/korean-english-park.dev.en'\n",
    "\n",
    "with open(val_k_file_path, 'r') as f:\n",
    "    val_ko_raw = f.read().splitlines()\n",
    "with open(val_e_file_path, 'r') as f:\n",
    "    val_en_raw = f.read().splitlines()\n",
    "\n",
    "val_cleaned_corpus  = clean_corpus(val_ko_raw, val_en_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_kor, val_eng = zip(*val_cleaned_corpus)\n",
    "val_ko_corpus = make_corpus(val_kor, ko_tokenizer)\n",
    "val_en_corpus = make_corpus(val_eng, en_tokenzier)\n",
    "\n",
    "val_en_ndarray = tf.keras.preprocessing.sequence.pad_sequences(val_en_corpus, maxlen=MAX_LENGTH,\n",
    "                                                  truncating='post',\n",
    "                                                  padding='post')\n",
    "val_ko_ndarray = tf.keras.preprocessing.sequence.pad_sequences(val_ko_corpus, maxlen=MAX_LENGTH,\n",
    "                                                  truncating='post',\n",
    "                                                  padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ko_ndarray.shape , val_en_ndarray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'enc_in' : en_ndarray ,\n",
    "                                                     'dec_in' :ko_ndarray},\n",
    "                                                    ko_ndarray)).batch(batch_size = BATCH_SIZE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({'enc_in' : val_en_ndarray , 'dec_in' :val_ko_ndarray}, val_ko_ndarray)).batch(batch_size = BATCH_SIZE)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(500).reshape(1,-1)\n",
    "np.zeros((100,10))\n",
    "for i in range(0, 100, 2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def get_angles(pos, i, d_model):\n",
    "        return   pos / 10000**(2*(i//2)/d_model)\n",
    "    pos_line = np.arange(pos).reshape(-1,1)\n",
    "    d_model_line = np.arange(d_model).reshape(1,-1)\n",
    "\n",
    "    temp_table = get_angles(pos_line, d_model_line, d_model)\n",
    "\n",
    "    sinusoid_table = np.zeros(temp_table.shape)\n",
    "\n",
    "    sinusoid_table[:,0::2] = tf.math.cos(temp_table[:, 0::2])\n",
    "    sinusoid_table[:,1::2] = tf.math.sin(temp_table[:, 1::2])\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(positional_encoding(4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, ko_tokenizer.pad_id()), 1.0, 0)\n",
    "    return seq[: , tf.newaxis, tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(tf.keras.layers.Layer):\n",
    "    self.depth  = 500\n",
    "\n",
    "    def scaled_dot_product_attention(q,k,v,mask):\n",
    "        matmul_qk = tf.matmul(q,k,transpose_b=True)\n",
    "        matmul_qk = matmul_qk / tf.math.sqrt(self.depth)\n",
    "        activation_score = tf.keras.activations.softmax(matmul_qk,axis=-1)\n",
    "        out = tf.matmul(activation_score ,v)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
